{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade torchvision\n"
      ],
      "metadata": {
        "id": "Usw-r-vbBG0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# 0) install extras (Torch+CUDA already in Colab)\n",
        "# -----------------------------------------------\n",
        "!pip install -q gdown albumentations opencv-python pycocotools tqdm\n",
        "\n",
        "# 1) mount Drive so we can save checkpoints\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2) download pcos_data.zip from Google Drive\n",
        "FILE_ID = \"1UBKaTpuDxOdrIE9iFjivWmbTpZsUCeMd\"   # <- your file ID\n",
        "!gdown --id $FILE_ID -O pcos_data.zip\n",
        "\n",
        "# 3) ensure clean extract folder, then unzip quietly & overwrite\n",
        "!rm -rf /content/data_split_v2\n",
        "!unzip -o -qq pcos_data.zip -d /content\n",
        "DATA_ROOT = \"/content/data_split_v2\"\n",
        "\n",
        "print(\"✅ Dataset ready at\", DATA_ROOT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLwjE6Y8qFkn",
        "outputId": "d8b2755f-a7e7-46a5-db43-4610ea1fc059"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1UBKaTpuDxOdrIE9iFjivWmbTpZsUCeMd\n",
            "From (redirected): https://drive.google.com/uc?id=1UBKaTpuDxOdrIE9iFjivWmbTpZsUCeMd&confirm=t&uuid=0c7db9c2-c71f-41a8-95af-a8a6a1d06440\n",
            "To: /content/pcos_data.zip\n",
            "100% 98.4M/98.4M [00:00<00:00, 242MB/s]\n",
            "✅ Dataset ready at /content/data_split_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────────\n",
        "# CELL 2: Fast sanity-check of your newly generated COCO JSONs\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "\n",
        "DATA_ROOT = \"/content/data_split_v2\"   # <-- same as in your unzip cell\n",
        "\n",
        "for split in (\"train\", \"val\", \"test\"):\n",
        "    ann_file = os.path.join(DATA_ROOT, \"annotations\", f\"{split}_coco.json\")\n",
        "    coco     = COCO(ann_file)\n",
        "    n_imgs   = len(coco.imgs)\n",
        "    n_anns   = len(coco.anns)\n",
        "    print(f\"{split:>5}: {n_imgs:4d} images, {n_anns:4d} GT boxes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVpjRZn33p6G",
        "outputId": "dd043e39-1d07-4d7f-9e51-3f74045bcd7b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "train: 2450 images, 1595 GT boxes\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "  val:  543 images, 1276 GT boxes\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            " test:  539 images,  972 GT boxes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ylSykJlSo7"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Dataset, Model (MobileNetV3-FPN) & Full Training + Pseudo-Mask Pipeline\n",
        "# with CLAHE preprocessing, improved pseudo-masking, and per-epoch validation + checkpointing\n",
        "# (evaluate_map50 safely handles missing stats)\n",
        "\n",
        "import os, cv2, json, numpy as np, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import Normalize\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.backbone_utils import mobilenet_backbone\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "# ────────────────────────────────────────────────────\n",
        "# CONFIGURATION\n",
        "# ────────────────────────────────────────────────────\n",
        "DATA_ROOT    = \"/content/data_split_v2\"\n",
        "BATCH_SIZE   = 8\n",
        "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMG_SIZE     = (256, 256)\n",
        "H, W         = IMG_SIZE\n",
        "NUM_CLASSES  = 2    # background + follicle\n",
        "HEAD_EPOCHS  = 10   # initial training before pseudo-masks\n",
        "FT_EPOCHS    = 20   # fine-tuning after pseudo-masks\n",
        "\n",
        "# Preprocessing: CLAHE + blur\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "\n",
        "# ImageNet normalization\n",
        "imagenet_norm = Normalize(mean=[0.485,0.456,0.406],\n",
        "                         std=[0.229,0.224,0.225])\n",
        "\n",
        "# ────────────────────────────────────────────────────\n",
        "# DATASET & DATALOADER\n",
        "# ────────────────────────────────────────────────────\n",
        "class FollicleSegDS(Dataset):\n",
        "    def __init__(self, root, ann_path, augment=False):\n",
        "        self.root    = root\n",
        "        self.coco    = COCO(ann_path)\n",
        "        self.ids     = list(self.coco.imgs.keys())\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        info = self.coco.loadImgs(self.ids[idx])[0]\n",
        "        path = os.path.join(self.root, info[\"file_name\"])\n",
        "\n",
        "        # 1) Load grayscale\n",
        "        gray = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # 2) CLAHE + blur\n",
        "        gray = clahe.apply(gray)\n",
        "        gray = cv2.GaussianBlur(gray, (5,5), 0)\n",
        "\n",
        "        # 3) Train-time augment grayscale\n",
        "        if self.augment:\n",
        "            # gamma correction\n",
        "            gamma = np.random.uniform(0.9, 1.1)\n",
        "            lut   = np.array([((i/255.0)**gamma)*255 for i in range(256)]).astype(np.uint8)\n",
        "            gray  = cv2.LUT(gray, lut)\n",
        "            # brightness/contrast\n",
        "            alpha = np.random.uniform(0.9,1.1); beta = np.random.uniform(-10,10)\n",
        "            gray  = np.clip(alpha*gray + beta, 0,255).astype(np.uint8)\n",
        "            # gaussian noise\n",
        "            noise = np.random.randn(*gray.shape)*5\n",
        "            gray  = np.clip(gray + noise, 0,255).astype(np.uint8)\n",
        "            # rotation ±15°\n",
        "            angle = np.random.uniform(-15,15)\n",
        "            M     = cv2.getRotationMatrix2D((gray.shape[1]/2,gray.shape[0]/2), angle, 1)\n",
        "            gray  = cv2.warpAffine(gray, M, (gray.shape[1],gray.shape[0]),\n",
        "                                   flags=cv2.INTER_NEAREST)\n",
        "\n",
        "        # 4) Resize & scale\n",
        "        img   = cv2.resize(gray, IMG_SIZE).astype(np.float32)/255.0\n",
        "        image = torch.from_numpy(img).unsqueeze(0).repeat(3,1,1)\n",
        "\n",
        "        # 5) Load masks & build boxes\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=info[\"id\"])\n",
        "        anns    = self.coco.loadAnns(ann_ids)\n",
        "        boxes, labels, masks = [], [], []\n",
        "        for a in anns:\n",
        "            m = self.coco.annToMask(a)\n",
        "            if self.augment:\n",
        "                m = cv2.warpAffine(m, M, (m.shape[1],m.shape[0]), flags=cv2.INTER_NEAREST)\n",
        "            m = cv2.resize(m, IMG_SIZE, cv2.INTER_NEAREST)\n",
        "            ys, xs = np.where(m>0)\n",
        "            if xs.size and ys.size:\n",
        "                x0,y0 = float(xs.min()), float(ys.min())\n",
        "                x1,y1 = float(xs.max()), float(ys.max())\n",
        "                x0,x1 = np.clip(x0,0,W-2), np.clip(x1,1,W-1)\n",
        "                y0,y1 = np.clip(y0,0,H-2), np.clip(y1,1,H-1)\n",
        "                if x1<=x0: x1=x0+1\n",
        "                if y1<=y0: y1=y0+1\n",
        "                boxes.append([x0,y0,x1,y1])\n",
        "                labels.append(1)\n",
        "                masks.append(torch.from_numpy(m.astype(np.uint8)))\n",
        "\n",
        "        target = {\n",
        "            \"boxes\":  torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0,4)),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64)   if labels else torch.empty((0,),dtype=torch.int64),\n",
        "            \"masks\":  torch.stack(masks)                        if masks else torch.empty((0,H,W),dtype=torch.uint8),\n",
        "            \"fname\":  info[\"file_name\"]\n",
        "        }\n",
        "\n",
        "        # 6) Final normalization\n",
        "        image = imagenet_norm(image)\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def get_loaders(root):\n",
        "    loaders = {}\n",
        "    for split in (\"train\",\"val\",\"test\"):\n",
        "        ds = FollicleSegDS(\n",
        "            root=os.path.join(root,split),\n",
        "            ann_path=os.path.join(root,\"annotations\",f\"{split}_coco.json\"),\n",
        "            augment=(split==\"train\")\n",
        "        )\n",
        "        loaders[split] = DataLoader(\n",
        "            ds, batch_size=BATCH_SIZE,\n",
        "            shuffle=(split==\"train\"),\n",
        "            num_workers=0, pin_memory=True,\n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "    return loaders\n",
        "\n",
        "# ────────────────────────────────────────────────────\n",
        "# MODEL INITIALIZATION (MobileNetV3-FPN + custom anchors)\n",
        "# ────────────────────────────────────────────────────\n",
        "backbone = mobilenet_backbone(\"mobilenet_v3_large\", pretrained=True, fpn=True)\n",
        "backbone.out_channels = 256\n",
        "\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=((32,),(64,),(128,)),\n",
        "    aspect_ratios=((0.5,1.0,2.0),)*3\n",
        ")\n",
        "\n",
        "model = MaskRCNN(\n",
        "    backbone=backbone,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    rpn_anchor_generator=anchor_generator\n",
        ").to(DEVICE)\n",
        "\n",
        "# ────────────────────────────────────────────────────\n",
        "# EVALUATION HELPER (AP50) – safe stats access\n",
        "# ────────────────────────────────────────────────────\n",
        "def evaluate_map50(model, data_root, device, img_size, score_thr=0.3):\n",
        "    annFile = os.path.join(data_root,\"annotations\",\"val_coco.json\")\n",
        "    imgDir  = os.path.join(data_root,\"val\")\n",
        "    coco_gt = COCO(annFile)\n",
        "    ids     = coco_gt.getImgIds()\n",
        "    results = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for img_id in ids:\n",
        "            info = coco_gt.loadImgs(img_id)[0]\n",
        "            img  = cv2.imread(os.path.join(imgDir, info[\"file_name\"]))\n",
        "            rgb  = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            im   = cv2.resize(rgb, img_size)\n",
        "            t    = torch.from_numpy(im.astype(\"float32\")/255.).permute(2,0,1).to(device)\n",
        "            t    = imagenet_norm(t)\n",
        "            out  = model([t])[0]\n",
        "            for box, sc, lb in zip(out[\"boxes\"].cpu().numpy(),\n",
        "                                   out[\"scores\"].cpu().numpy(),\n",
        "                                   out[\"labels\"].cpu().numpy()):\n",
        "                if sc < score_thr: continue\n",
        "                x0,y0,x1,y1 = box\n",
        "                results.append({\n",
        "                    \"image_id\":    img_id,\n",
        "                    \"category_id\": int(lb),\n",
        "                    \"bbox\":        [float(x0),float(y0),float(x1-x0),float(y1-y0)],\n",
        "                    \"score\":       float(sc)\n",
        "                })\n",
        "    resFile = \"/content/val_results.json\"\n",
        "    with open(resFile, \"w\") as f: json.dump(results, f)\n",
        "    coco_dt   = coco_gt.loadRes(resFile)\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
        "    coco_eval.params.imgIds = ids\n",
        "    coco_eval.evaluate(); coco_eval.accumulate()\n",
        "    stats = getattr(coco_eval, \"stats\", [])\n",
        "    if isinstance(stats, (list, np.ndarray)) and len(stats) > 1:\n",
        "        return float(stats[1])\n",
        "    if isinstance(stats, (list, np.ndarray)) and len(stats) > 0:\n",
        "        return float(stats[0])\n",
        "    return 0.0\n",
        "\n",
        "# ────────────────────────────────────────────────────\n",
        "# TRAINING: HEAD TRAIN, PSEUDO-MASK, FINE-TUNE + CHECKPOINT\n",
        "# ────────────────────────────────────────────────────\n",
        "loaders = get_loaders(DATA_ROOT)\n",
        "\n",
        "# 1) Initial training (HEAD_EPOCHS) on all parameters\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=HEAD_EPOCHS)\n",
        "scaler    = GradScaler()\n",
        "\n",
        "def non_empty(loader):\n",
        "    for imgs, tgts in loader:\n",
        "        if any(t[\"boxes\"].numel()>0 for t in tgts):\n",
        "            yield imgs, tgts\n",
        "\n",
        "for ep in range(1, HEAD_EPOCHS+1):\n",
        "    model.train(); total=0.0; cnt=0\n",
        "    for imgs, tgts in non_empty(loaders[\"train\"]):\n",
        "        imgs    = [i.to(DEVICE) for i in imgs]\n",
        "        targets = [{\"boxes\":t[\"boxes\"].to(DEVICE),\n",
        "                    \"labels\":t[\"labels\"].to(DEVICE),\n",
        "                    \"masks\":t[\"masks\"].to(DEVICE)} for t in tgts]\n",
        "        with autocast(device_type='cuda'):\n",
        "            loss = sum(model(imgs, targets).values())\n",
        "        scaler.scale(loss).backward()\n",
        "        clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n",
        "        total+=loss.item(); cnt+=1\n",
        "    print(f\"Epoch {ep}/{HEAD_EPOCHS}  loss={total/cnt:.4f}\")\n",
        "    scheduler.step()\n",
        "\n",
        "# 2) Pseudo-mask generation (threshold=0.5)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, tgts in non_empty(loaders[\"train\"]):\n",
        "        outs = model([i.to(DEVICE) for i in imgs])\n",
        "        for t, out in zip(tgts, outs):\n",
        "            keep = out['scores'] > 0.5\n",
        "            for i, m in enumerate(out['masks'][keep,0].cpu().numpy()):\n",
        "                fname = t[\"fname\"].replace(\".jpg\", f\"_ps{i}.png\")\n",
        "                cv2.imwrite(os.path.join(DATA_ROOT, \"train\", fname),\n",
        "                            (m>0.5).astype(np.uint8)*255)\n",
        "# reload with pseudo-masks\n",
        "loaders = get_loaders(DATA_ROOT)\n",
        "\n",
        "# 3) Fine-tuning (FT_EPOCHS) with validation checkpointing\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=FT_EPOCHS)\n",
        "best_ap50 = 0.0\n",
        "\n",
        "for ep in range(1, FT_EPOCHS+1):\n",
        "    model.train(); total=0.0; cnt=0\n",
        "    for imgs, tgts in non_empty(loaders[\"train\"]):\n",
        "        imgs    = [i.to(DEVICE) for i in imgs]\n",
        "        targets = [{\"boxes\":t[\"boxes\"].to(DEVICE),\n",
        "                    \"labels\":t[\"labels\"].to(DEVICE),\n",
        "                    \"masks\":t[\"masks\"].to(DEVICE)} for t in tgts]\n",
        "        with autocast(device_type='cuda'):\n",
        "            loss = sum(model(imgs, targets).values())\n",
        "        scaler.scale(loss).backward()\n",
        "        clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n",
        "        total+=loss.item(); cnt+=1\n",
        "    scheduler.step()\n",
        "    train_loss = total/cnt\n",
        "    val_ap50   = evaluate_map50(model, DATA_ROOT, DEVICE, IMG_SIZE, score_thr=0.3)\n",
        "    print(f\"Fine Epoch {ep}/{FT_EPOCHS}  train_loss={train_loss:.4f}  val_AP50={val_ap50:.3f}\")\n",
        "    if val_ap50 > best_ap50:\n",
        "        best_ap50 = val_ap50\n",
        "        torch.save(model.state_dict(), \"/content/best_maskrcnn.pth\")\n",
        "        print(f\" → New best AP50: {best_ap50:.3f}\")\n",
        "\n",
        "print(\"✅ Training + pseudo-mask + validation complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, json, torch\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "# Paths\n",
        "annFile = os.path.join(DATA_ROOT, \"annotations\", \"val_coco.json\")\n",
        "resFile = \"/content/val_detections.json\"\n",
        "IMG_DIR = os.path.join(DATA_ROOT, \"val\")\n",
        "\n",
        "# Load ground truth\n",
        "coco_gt = COCO(annFile)\n",
        "img_ids = coco_gt.getImgIds()\n",
        "\n",
        "model.eval()\n",
        "results = []\n",
        "\n",
        "for img_id in img_ids:\n",
        "    info = coco_gt.loadImgs(img_id)[0]\n",
        "    img_path = os.path.join(IMG_DIR, info[\"file_name\"])\n",
        "    orig     = cv2.imread(img_path)\n",
        "    rgb      = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
        "    resized  = cv2.resize(rgb, IMG_SIZE)\n",
        "\n",
        "    # create a [3,H,W] tensor, NOT [1,3,H,W]\n",
        "    tensor = torch.from_numpy(resized.astype(\"float32\")/255.0).permute(2,0,1).to(DEVICE)\n",
        "    tensor = imagenet_norm(tensor)   # still 3×H×W\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model([tensor])[0]      # list of one 3D tensor\n",
        "\n",
        "    boxes  = out[\"boxes\"].cpu().numpy()\n",
        "    scores = out[\"scores\"].cpu().numpy()\n",
        "    labels = out[\"labels\"].cpu().numpy()\n",
        "\n",
        "    for box,score,label in zip(boxes,scores,labels):\n",
        "        x0,y0,x1,y1 = box\n",
        "        results.append({\n",
        "            \"image_id\":    img_id,\n",
        "            \"category_id\": int(label),\n",
        "            \"bbox\":        [float(x0), float(y0), float(x1-x0), float(y1-y0)],\n",
        "            \"score\":       float(score),\n",
        "        })\n",
        "\n",
        "# write and evaluate\n",
        "with open(resFile, \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "\n",
        "coco_dt   = coco_gt.loadRes(resFile)\n",
        "coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
        "coco_eval.params.imgIds = img_ids\n",
        "coco_eval.evaluate()\n",
        "coco_eval.accumulate()\n",
        "coco_eval.summarize()\n"
      ],
      "metadata": {
        "id": "uQVDT2zGqDM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}